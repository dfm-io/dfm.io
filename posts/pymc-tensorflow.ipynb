{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Title: PyMC3 + TensorFlow\n",
    "Date: 2018-08-02\n",
    "Category: Data Analysis\n",
    "Slug: pymc-tensorflow\n",
    "Summary: the most ambitious crossover event in history\n",
    "Math: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams[\"savefig.dpi\"] = 100\n",
    "rcParams[\"figure.dpi\"] = 100\n",
    "rcParams[\"font.size\"] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I will describe a hack that let's us use [PyMC3](http://docs.pymc.io/) to sample a probability density defined using [TensorFlow](https://www.tensorflow.org/).\n",
    "This isn't necessarily a Good Ideaâ„¢, but I've found it useful for a few projects so I wanted to share the method.\n",
    "To start, I'll try to motivate why I decided to attempt this mashup, and then I'll give a simple example to demonstrate how you might use this technique in your own work.\n",
    "\n",
    "## Why TensorFlow?\n",
    "\n",
    "I recently started using TensorFlow as a framework for probabilistic modeling (and [encouraging other astronomers to do the same](https://speakerdeck.com/dfm/tensorflow-for-astronomers)) because the API seemed stable and it was relatively easy to extend the language with custom operations written in C++.\n",
    "This second point is crucial in astronomy because we often want to fit realistic, physically motivated models to our data, and it can be inefficient to implement these algorithms within the confines of existing probabilistic programming languages.\n",
    "To this end, I have been working on developing various custom operations within TensorFlow to implement [scalable Gaussian processes](https://github.com/dfm/celeriteflow) and [various special functions for fitting exoplanet data (Foreman-Mackey et al., in prep, ha!)](https://github.com/dfm/exoplanet).\n",
    "These experiments have yielded promising results, but my ultimate goal has always been to combine these models with [Hamiltonian Monte Carlo](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo) sampling to perform posterior inference.\n",
    "I don't know of any Python packages with the capabilities of projects like PyMC3 or [Stan](http://mc-stan.org/) that support TensorFlow out of the box.\n",
    "I know that [Edward](http://edwardlib.org/)/[TensorFlow probability](https://github.com/tensorflow/probability) has an HMC sampler, but it does not have a NUTS implementation, tuning heuristics, or any of the other niceties that the MCMC-first libraries provide.\n",
    "\n",
    "## Why HMC?\n",
    "\n",
    "The benefit of HMC compared to some other MCMC methods (including [one that I wrote](http://emcee.readthedocs.io/en/stable/)) is that it is substantially more efficient (i.e. requires less computation time per independent sample) for models with large numbers of parameters.\n",
    "To achieve this efficiency, the sampler uses the gradient of the log probability function with respect to the parameters to generate good proposals.\n",
    "This means that it must be possible to compute the first derivative of your model with respect to the input parameters.\n",
    "To do this in a user-friendly way, most popular inference libraries provide a modeling framework that users must use to implement their model and then the code can [automatically compute these derivatives](https://en.wikipedia.org/wiki/Automatic_differentiation).\n",
    "\n",
    "## Why PyMC3?\n",
    "\n",
    "As far as I can tell, there are two popular libraries for HMC inference in Python: [PyMC3](http://docs.pymc.io/) and [Stan](http://mc-stan.org/) (via the [pystan](http://pystan.readthedocs.io/en/latest/) interface).\n",
    "I have previously blogged about [extending Stan using custom C++ code and a forked version of pystan](https://dfm.io/posts/stan-c++/), but I haven't actually been able to use this method for my research because debugging any code more complicated than the one in that example ended up being far too tedious.\n",
    "Furthermore, since I generally want to do my initial tests and make my plots in Python, I always ended up implementing two version of my model (one in Stan and one in Python) and it was frustrating to make sure that these always gave the same results.\n",
    "PyMC3 is much more appealing to me because the models are actually Python objects so you can use the same implementation for sampling and pre/post-processing.\n",
    "The catch with PyMC3 is that you must be able to evaluate your model within the [Theano](http://deeplearning.net/software/theano/) framework and I wasn't so keen to learn Theano when I had already invested a substantial amount of time into TensorFlow and [since Theano has been deprecated](https://groups.google.com/forum/#!msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ) as a general purpose modeling language.\n",
    "What I *really* want is a sampling engine that does all the tuning like PyMC3/Stan, but without requiring the use of a specific modeling framework.\n",
    "I imagine that this interface would accept two Python functions (one that evaluates the log probability, and one that evaluates its gradient) and then the user could choose whichever modeling stack they want.\n",
    "That being said, my dream sampler doesn't exist (despite [my weak attempt](https://github.com/dfm/hemcee) to start developing it) so I decided to see if I could hack PyMC3 to do what I wanted.\n",
    "\n",
    "## The TensorFlow + Theano mashup\n",
    "\n",
    "To get started on implementing this, I reached out to [Thomas Wiecki](https://twitter.com/twiecki) (one of the lead developers of PyMC3 [who has written about a similar MCMC mashups](http://twiecki.github.io/blog/2013/09/23/emcee-pymc/)) for tips,\n",
    "He came back with a few excellent suggestions, but the one that really stuck out was to \"...write your logp/dlogp as a theano op that you then use in your (very simple) model definition\".\n",
    "The basic idea here is that, since PyMC3 models are implemented using Theano, it should be possible to write an extension to Theano that knows how to call TensorFlow.\n",
    "Then, this extension could be integrated seamlessly into the model.\n",
    "The two key pages of documentation are the [Theano docs for writing custom operations (ops)](http://deeplearning.net/software/theano/extending/extending_theano.html) and the [PyMC3 docs for using these custom ops](https://docs.pymc.io/advanced_theano.html#writing-custom-theano-ops).\n",
    "After starting on this project, I also discovered [an issue on GitHub with a similar goal](https://github.com/pymc-devs/pymc3/issues/804) that ended up being very helpful.\n",
    "\n",
    "Based on these docs, my complete implementation for a custom Theano op that calls TensorFlow is given below.\n",
    "This implemetation requires two `theano.tensor.Op` subclasses, one for the operation itself (`TensorFlowOp`) and one for the gradient operation (`_TensorFlowGradOp`).\n",
    "Like Theano, TensorFlow has support for reverse-mode automatic differentiation, so we can use the [`tf.gradients`](https://www.tensorflow.org/api_docs/python/tf/gradients) function to provide the gradients for the op."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "session = tf.get_default_session()\n",
    "if session is None:\n",
    "    session = tf.InteractiveSession()\n",
    "\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "\n",
    "\n",
    "def _to_tensor_type(shape):\n",
    "    return tt.TensorType(dtype=\"float64\", broadcastable=[False] * len(shape))\n",
    "\n",
    "\n",
    "class TensorFlowOp(tt.Op):\n",
    "    \"\"\"A custom Theano Op uses TensorFlow as the computation engine\n",
    "\n",
    "    Args:\n",
    "        target (Tensor): The TensorFlow tensor defining the output of\n",
    "            this operation\n",
    "        parameters (list(Tensor)): A list of TensorFlow tensors that\n",
    "            are inputs to this operation\n",
    "        names (Optional(list)): A list of names for the parameters.\n",
    "            These are the names that will be used within PyMC3\n",
    "        feed_dict (Optional(dict)): A \"feed_dict\" that is provided to\n",
    "            the TensorFlow session when the operation is executed\n",
    "        session (Optional): A TensorFlow session that can be used to\n",
    "            evaluate the operation\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target, parameters, names=None, feed_dict=None, session=None):\n",
    "        self.parameters = parameters\n",
    "        self.names = names\n",
    "        self._feed_dict = dict() if feed_dict is None else feed_dict\n",
    "        self._session = session\n",
    "        self.target = target\n",
    "\n",
    "        # Execute the operation once to work out the shapes of the\n",
    "        # parameters and the target\n",
    "        in_values, out_value = self.session.run(\n",
    "            [self.parameters, self.target], feed_dict=self._feed_dict\n",
    "        )\n",
    "        self.shapes = [np.shape(v) for v in in_values]\n",
    "        self.output_shape = np.shape(out_value)\n",
    "\n",
    "        # Based on this result, work out the shapes that the Theano op\n",
    "        # will take in and return\n",
    "        self.itypes = tuple([_to_tensor_type(shape) for shape in self.shapes])\n",
    "        self.otypes = tuple([_to_tensor_type(self.output_shape)])\n",
    "\n",
    "        # Build another custom op to represent the gradient (see below)\n",
    "        self._grad_op = _TensorFlowGradOp(self)\n",
    "\n",
    "    @property\n",
    "    def session(self):\n",
    "        \"\"\"The TensorFlow session associated with this operation\"\"\"\n",
    "        if self._session is None:\n",
    "            self._session = tf.get_default_session()\n",
    "        return self._session\n",
    "\n",
    "    def get_feed_dict(self, sample):\n",
    "        \"\"\"Get the TensorFlow feed_dict for a given sample\n",
    "\n",
    "        This method will only work when a value for ``names`` was provided\n",
    "        during instantiation.\n",
    "\n",
    "        sample (dict): The specification of a specific sample in the chain\n",
    "\n",
    "        \"\"\"\n",
    "        if self.names is None:\n",
    "            raise RuntimeError(\"'names' must be set in order to get the feed_dict\")\n",
    "        return dict(\n",
    "            ((param, sample[name]) for name, param in zip(self.names, self.parameters)),\n",
    "            **self._feed_dict\n",
    "        )\n",
    "\n",
    "    def infer_shape(self, node, shapes):\n",
    "        \"\"\"A required method that returns the shape of the output\"\"\"\n",
    "        return (self.output_shape,)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        \"\"\"A required method that actually executes the operation\"\"\"\n",
    "        # To execute the operation using TensorFlow we must map the inputs from\n",
    "        # Theano to the TensorFlow parameter using a \"feed_dict\"\n",
    "        feed_dict = dict(zip(self.parameters, inputs), **self._feed_dict)\n",
    "        outputs[0][0] = np.array(self.session.run(self.target, feed_dict=feed_dict))\n",
    "\n",
    "    def grad(self, inputs, gradients):\n",
    "        \"\"\"A method that returns Theano op to compute the gradient\n",
    "\n",
    "        In this case, we use another custom op (see the definition below).\n",
    "\n",
    "        \"\"\"\n",
    "        op = self._grad_op(*(inputs + gradients))\n",
    "        # This hack seems to be required for ops with a single input\n",
    "        if not isinstance(op, (list, tuple)):\n",
    "            return [op]\n",
    "        return op\n",
    "\n",
    "\n",
    "class _TensorFlowGradOp(tt.Op):\n",
    "    \"\"\"A custom Theano Op defining the gradient of a TensorFlowOp\n",
    "\n",
    "    Args:\n",
    "        base_op (TensorFlowOp): The original Op\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_op):\n",
    "        self.base_op = base_op\n",
    "\n",
    "        # Build the TensorFlow operation to apply the reverse mode\n",
    "        # autodiff for this operation\n",
    "        # The placeholder is used to include the gradient of the\n",
    "        # output as a seed\n",
    "        self.dy = tf.placeholder(tf.float64, base_op.output_shape)\n",
    "        self.grad_target = tf.gradients(\n",
    "            base_op.target, base_op.parameters, grad_ys=self.dy\n",
    "        )\n",
    "\n",
    "        # This operation will take the original inputs and the gradient\n",
    "        # seed as input\n",
    "        types = [_to_tensor_type(shape) for shape in base_op.shapes]\n",
    "        self.itypes = tuple(types + [_to_tensor_type(base_op.output_shape)])\n",
    "        self.otypes = tuple(types)\n",
    "\n",
    "    def infer_shape(self, node, shapes):\n",
    "        return self.base_op.shapes\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        feed_dict = dict(\n",
    "            zip(self.base_op.parameters, inputs[:-1]), **self.base_op._feed_dict\n",
    "        )\n",
    "        feed_dict[self.dy] = inputs[-1]\n",
    "        result = self.base_op.session.run(self.grad_target, feed_dict=feed_dict)\n",
    "        for i, r in enumerate(result):\n",
    "            outputs[i][0] = np.array(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test that our op works for some simple test cases.\n",
    "For example, we can add a simple (read: silly) op that uses TensorFlow to perform an elementwise square of a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theano.tests import unittest_tools as utt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the operation in TensorFlow\n",
    "x = tf.Variable(np.random.randn(5), dtype=tf.float64)\n",
    "sq = tf.square(x)\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# Define the Theano op\n",
    "square_op = TensorFlowOp(sq, [x])\n",
    "\n",
    "# Test that the gradient is correct\n",
    "pt = session.run(square_op.parameters)\n",
    "utt.verify_grad(square_op, pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is obviously a silly example because Theano already has this functionality, but this can also be generalized to more complicated models.\n",
    "\n",
    "This `TensorFlowOp` implementation will be sufficient for our purposes, but it has some limitations including:\n",
    "\n",
    "1. By design, the output of the operation must be a single tensor. It shouldn't be too hard to generalize this to multiple outputs if you need to, but I haven't tried.\n",
    "2. The input and output variables must have fixed dimensions. When the `TensorFlowOp` is initialized, the input and output tensors will be evaluated using the current TensorFlow session to work out the shapes.\n",
    "3. etc., I'm sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example\n",
    "\n",
    "For this demonstration, we'll fit a *very* simple model that would actually be much easier to just fit using vanilla PyMC3, but it'll still be useful for demonstrating what we're trying to do.\n",
    "We'll fit a line to data with the likelihood function:\n",
    "\n",
    "$$\n",
    "p(\\{y_n\\}\\,|\\,m,\\,b,\\,s) = \\prod_{n=1}^N \\frac{1}{\\sqrt{2\\,\\pi\\,s^2}}\\,\\exp\\left(-\\frac{(y_n-m\\,x_n-b)^2}{s^2}\\right)\n",
    "$$\n",
    "\n",
    "where $m$, $b$, and $s$ are the parameters.\n",
    "We'll choose uniform priors on $m$ and $b$, and a log-uniform prior for $s$.\n",
    "\n",
    "To get started, generate some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "true_params = np.array([0.5, -2.3, -0.23])\n",
    "\n",
    "N = 50\n",
    "t = np.linspace(0, 10, 2)\n",
    "x = np.random.uniform(0, 10, 50)\n",
    "y = x * true_params[0] + true_params[1]\n",
    "y_obs = y + np.exp(true_params[-1]) * np.random.randn(N)\n",
    "\n",
    "plt.plot(x, y_obs, \".k\", label=\"observations\")\n",
    "plt.plot(t, true_params[0] * t + true_params[1], label=\"truth\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the log-likelihood function in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_tensor = tf.Variable(0.0, dtype=tf.float64, name=\"m\")\n",
    "b_tensor = tf.Variable(0.0, dtype=tf.float64, name=\"b\")\n",
    "logs_tensor = tf.Variable(0.0, dtype=tf.float64, name=\"logs\")\n",
    "\n",
    "t_tensor = tf.constant(t, dtype=tf.float64)\n",
    "x_tensor = tf.constant(x, dtype=tf.float64)\n",
    "y_tensor = tf.constant(y_obs, dtype=tf.float64)\n",
    "\n",
    "mean = m_tensor * x_tensor + b_tensor\n",
    "pred = m_tensor * t_tensor + b_tensor\n",
    "\n",
    "loglike = -0.5 * tf.reduce_sum(tf.square(y_tensor - mean)) * tf.exp(-2 * logs_tensor)\n",
    "loglike -= 0.5 * N * logs_tensor\n",
    "\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can fit for the maximum likelihood parameters using an optimizer from TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [m_tensor, b_tensor, logs_tensor]\n",
    "opt = tf.contrib.opt.ScipyOptimizerInterface(-loglike, params)\n",
    "opt.minimize(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the maximum likelihood solution compared to the data and the true relation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y_obs, \".k\", label=\"observations\")\n",
    "plt.plot(t, true_params[0] * t + true_params[1], label=\"truth\")\n",
    "plt.plot(t, pred.eval(), label=\"max.\\ like.\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use PyMC3 to generate posterior samples for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "\n",
    "# First, expose the TensorFlow log likelihood implementation to Theano\n",
    "# so that PyMC3 can use it\n",
    "# NOTE: The \"names\" parameter refers to the names that will be used in\n",
    "# in the PyMC3 model (see below)\n",
    "tf_loglike = TensorFlowOp(\n",
    "    loglike, [m_tensor, b_tensor, logs_tensor], names=[\"m\", \"b\", \"logs\"]\n",
    ")\n",
    "\n",
    "# Test the gradient\n",
    "pt = session.run(tf_loglike.parameters)\n",
    "utt.verify_grad(tf_loglike, pt)\n",
    "\n",
    "# Set up the model as usual\n",
    "with pm.Model() as model:\n",
    "    # Uniform priors on all the parameters\n",
    "    m = pm.Uniform(\"m\", -5, 5)\n",
    "    b = pm.Uniform(\"b\", -5, 5)\n",
    "    logs = pm.Uniform(\"logs\", -5, 5)\n",
    "\n",
    "    # Define a custom \"potential\" to calculate the log likelihood\n",
    "    pm.Potential(\"loglike\", tf_loglike(m, b, logs))\n",
    "\n",
    "    # NOTE: You *must* use \"cores=1\" because TensorFlow can't deal\n",
    "    # with being pickled!\n",
    "    trace = pm.sample(1000, tune=2000, cores=1, nuts_kwargs=dict(target_accept=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sampling, we can make the usual diagnostic plots.\n",
    "First, the trace plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the [\"corner\" plot](http://corner.readthedocs.io):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://corner.readthedocs.io\n",
    "import corner\n",
    "\n",
    "samples = np.vstack([trace[k].flatten() for k in [\"m\", \"b\", \"logs\"]]).T\n",
    "corner.corner(samples, labels=[\"m\", \"b\", \"log(s)\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the posterior predictions for the line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y_obs, \".k\", label=\"observations\")\n",
    "\n",
    "for j in np.random.randint(len(trace), size=25):\n",
    "    feed_dict = tf_loglike.get_feed_dict(trace[j])\n",
    "    plt.plot(t, pred.eval(feed_dict=feed_dict), color=\"C1\", alpha=0.3)\n",
    "\n",
    "plt.plot(t, true_params[0] * t + true_params[1], label=\"truth\")\n",
    "plt.plot([], [], color=\"C1\", label=\"post.\\ samples\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this post, I demonstrated a hack that allows us to use PyMC3 to sample a model defined using TensorFlow.\n",
    "This might be useful if you already have an implementation of your model in TensorFlow and don't want to learn how to port it it Theano, but it also presents an example of the small amount of work that is required to support non-standard probabilistic modeling languages with PyMC3.\n",
    "It should be possible (easy?) to implement something similar for [TensorFlow probability](https://github.com/tensorflow/probability), [PyTorch](https://pytorch.org/), [autograd](https://github.com/HIPS/autograd), or any of your other favorite modeling frameworks.\n",
    "\n",
    "I hope that you find this useful in your research and [don't forget to cite PyMC3](https://doi.org/10.7717/peerj-cs.55) in all your papers. Thanks for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
