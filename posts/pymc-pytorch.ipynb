{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Title: PyMC3 + PyTorch\n",
    "Date: 2019-10-15\n",
    "Category: Data Analysis\n",
    "Slug: pymc-pytorch\n",
    "Summary: the second most ambitious crossover event in history\n",
    "Math: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post is a small extension to [my previous post](../pymc-tensorflow) where I demonstrated that it was possible to combine TensorFlow with PyMC3 to take advantage of the modeling capabilities of TensorFlow while still using the powerful inference engine provided by PyMC3.\n",
    "The basic procedure involved writing a custom Theano operation that understood how to evaluate a TensorFlow tensor.\n",
    "In this post, I provide a similar snippet that can be used to combine PyTorch and PyMC3 to a similar end.\n",
    "\n",
    "One reason why I'm interested in these experiments is because I want to use these tools in a fundamentally different way than some other users.\n",
    "In particular, I spend a lot of my time writing custom ops to extend the modeling languages provided by the standard model building languages since the physically motivated models that I'm interested in fitting often aren't easily or efficiently implemented using the existing stack.\n",
    "For example, I recently release the [\"exoplanet\" library](http://exoplanet.dfm.io/en/stable/) which is an extension to PyMC3 that provides much of the custom functionality needed for fitting astronomical time series data sets.\n",
    "I chose PyMC3 even though I knew that Theano was deprecated because I found that it had the best combination of powerful inference capabilities and an extensible interface.\n",
    "With the development of PyMC4, it's not clear that my use case will be well supported since the sampling will be so tightly embedded in TensorFlow.\n",
    "Furthermore, I don't want to be locked into using TensorFlow just so that I can take advantage of PyMC4's inference algorithms.\n",
    "Instead, I'm interested in understanding how feasible it would be to fork the inference engine part of PyMC3 to build a Python-based inference library that could be used for inference with models defined in TensorFlow-probability, Pyro, Jax, or whatever comes next without a full re-write.\n",
    "My key thought here is that I don't think that the modeling and inference components of a probabilistic modeling stack need to be as tightly integrated as most existing packages are.\n",
    "\n",
    "This post doesn't answer the question, but I thought that it was worth sharing anyways.\n",
    "As in the TensorFlow post, I'll demonstrate this idea using linear regression. \n",
    "Take a look [at the previous post](../pymc-tensorflow) for more details.\n",
    "First, let's generate the same simulated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "true_params = np.array([0.5, -2.3, -0.23])\n",
    "\n",
    "N = 50\n",
    "t = np.linspace(0, 10, 2)\n",
    "x = np.random.uniform(0, 10, 50)\n",
    "y = x * true_params[0] + true_params[1]\n",
    "y_obs = y + np.exp(true_params[-1]) * np.random.randn(N)\n",
    "\n",
    "plt.plot(x, y_obs, \".k\", label=\"observations\")\n",
    "plt.plot(t, true_params[0] * t + true_params[1], label=\"truth\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's implement the likelihood function for this simple linear model using PyTorch.\n",
    "\n",
    "**Disclaimer:** I'm definitely not a PyTorch expert so there might be a better way to do this, but this should at least demonstrate the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "\n",
    "        self.m = torch.nn.Parameter(torch.tensor(0.0, dtype=torch.float64))\n",
    "        self.b = torch.nn.Parameter(torch.tensor(0.0, dtype=torch.float64))\n",
    "        self.logs = torch.nn.Parameter(torch.tensor(0.0, dtype=torch.float64))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        mean = self.m * x + self.b\n",
    "        loglike = -0.5 * torch.sum(\n",
    "            (y - mean) ** 2 * torch.exp(-2 * self.logs) + 2 * self.logs\n",
    "        )\n",
    "        return loglike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, here's a custom Theano Op that knows how to evaluate a PyTorch *scalar* and its gradient.\n",
    "It would be possible to extend this work for arbitrary PyTorch tensors, but it would take a bit more book keeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as tt\n",
    "\n",
    "\n",
    "class TorchOp(tt.Op):\n",
    "    def __init__(self, module, params, args=None):\n",
    "        self.module = module\n",
    "        self.params = list(params)\n",
    "        if args is None:\n",
    "            self.args = tuple()\n",
    "        else:\n",
    "            self.args = tuple(args)\n",
    "\n",
    "    def make_node(self, *args):\n",
    "        if len(args) != len(self.params):\n",
    "            raise ValueError(\"dimension mismatch\")\n",
    "        args = [tt.as_tensor_variable(a) for a in args]\n",
    "        return theano.graph.basic.Apply(\n",
    "            self, args, [tt.dscalar().type()] + [a.type() for a in args]\n",
    "        )\n",
    "\n",
    "    def infer_shape(self, *args):\n",
    "        shapes = args[-1]\n",
    "        return tuple([()] + list(shapes))\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        for p, value in zip(self.params, inputs):\n",
    "            p.data = torch.tensor(value)\n",
    "            if p.grad is not None:\n",
    "                p.grad.detach_()\n",
    "                p.grad.zero_()\n",
    "\n",
    "        result = self.module(*self.args)\n",
    "        result.backward()\n",
    "\n",
    "        outputs[0][0] = result.detach().numpy()\n",
    "        for i, p in enumerate(self.params):\n",
    "            outputs[i + 1][0] = p.grad.numpy()\n",
    "\n",
    "    def grad(self, inputs, gradients):\n",
    "        for i, g in enumerate(gradients[1:]):\n",
    "            if not isinstance(g.type, theano.gradient.DisconnectedType):\n",
    "                raise ValueError(\n",
    "                    \"can't propagate gradients wrt parameter {0}\".format(i + 1)\n",
    "                )\n",
    "        return [gradients[0] * d for d in self(*inputs)[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we can combine these into a Theano op that knows how to evaluate the linear model using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the PyTorch model\n",
    "model = torch.jit.script(LinearModel())\n",
    "\n",
    "# It's useful to select the parameters directly instead of using model.parameters()\n",
    "# so that we make sure that the order is as expected\n",
    "params = [model.m, model.b, model.logs]\n",
    "\n",
    "# The \"forward\" method of the torch op takes the data as well\n",
    "args = [torch.tensor(x, dtype=torch.double), torch.tensor(y_obs, dtype=torch.double)]\n",
    "\n",
    "# Finally put it all together\n",
    "op = TorchOp(model, params, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're all set to use this in a PyMC3 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "\n",
    "with pm.Model() as torch_model:\n",
    "\n",
    "    m = pm.Uniform(\"m\", -5, 5)\n",
    "    b = pm.Uniform(\"b\", -5, 5)\n",
    "    logs = pm.Uniform(\"logs\", -5, 5)\n",
    "\n",
    "    pm.Potential(\"obs\", op(m, b, logs)[0])\n",
    "\n",
    "    np.random.seed(6940)\n",
    "    torch_trace = pm.sample(\n",
    "        1000, tune=5000, target_accept=0.9, return_inferencedata=False, cores=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that unlike the TensorFlow implementation, there's no problem using multiple cores with this model and we actually get more than a factor of two increase in efficiency (probably because PyTorch has less Python overhead per call).\n",
    "\n",
    "For comparison, we can reimplement the same model directly in PyMC3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as pymc3_model:\n",
    "\n",
    "    m = pm.Uniform(\"m\", -5, 5)\n",
    "    b = pm.Uniform(\"b\", -5, 5)\n",
    "    logs = pm.Uniform(\"logs\", -5, 5)\n",
    "\n",
    "    pm.Normal(\"obs\", mu=m * x + b, sd=pm.math.exp(logs), observed=y_obs)\n",
    "\n",
    "    np.random.seed(6940)\n",
    "    pymc3_trace = pm.sample(\n",
    "        1000, tune=5000, target_accept=0.9, return_inferencedata=False, cores=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the PyMC3 model is about a factor of 2 faster than the PyTorch model, but this is a simple enough model that it's not really a fair comparison.\n",
    "I expect that this gap would close for more expensive models where the overhead is less important.\n",
    "Personally, I'm willing to pay some performance penalty for the benefit of being able to use whichever modeling framework I want without serious compromises when it comes to inference capabilities.\n",
    "\n",
    "Finally, we can confirm that we got the same results (they should actually be *identical* because I used the same random number seed above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(torch_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(pymc3_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks right to me!\n",
    "\n",
    "I think that this experiment seems pretty promising.\n",
    "It demonstrates that it is feasible to sample a model implemented in PyTorch using PyMC3 with some overhead.\n",
    "I expect that more expensive models like the ones I normally work on will be even more closely matched in terms of performance.\n",
    "There are still some open questions, but I think that there's enough here to sketch out a plan for a common Python inference library that could be used with models defined in any modeling framework that can be called from within Python.\n",
    "To zeroth order, I would like something that can do NUTS sampling where the interface is just a Python function that computes the log-probability and its derivative for a vector of parameters (and maybe some other functions for evaluating deterministics, etc.).\n",
    "In the long term, it would be awesome to reproduce other functionality of PyMC3 like ADVI, but I have less experience with that so I'm not sure exactly what that would take.\n",
    "Either way, I'd love to hear if you have any feedback, tips, questions, pictures of good dogs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
